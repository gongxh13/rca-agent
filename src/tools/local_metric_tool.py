"""
Local Metric Analysis Tool

Concrete implementation of MetricAnalysisTool for the OpenRCA dataset.
Uses local CSV files via OpenRCADataLoader to provide metric analysis.
"""

from typing import Any, Dict, List, Optional, Literal
import pandas as pd
import numpy as np
from datetime import datetime
import json

from .metric_tool import MetricAnalysisTool
from .data_loader import OpenRCADataLoader
from src.utils.time_utils import to_iso_shanghai

try:
    import ruptures as rpt
    RUPTURES_AVAILABLE = True
except ImportError:
    RUPTURES_AVAILABLE = False


class LocalMetricAnalysisTool(MetricAnalysisTool):
    """
    Local implementation of MetricAnalysisTool using OpenRCA dataset files.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize the local metric tool.
        
        Args:
            config: Configuration dictionary containing:
                   - dataset_path: Path to the OpenRCA dataset root
        """
        super().__init__(config)
        self.data_loader: Optional[OpenRCADataLoader] = None
        
    def initialize(self) -> None:
        """Initialize the data loader."""
        super().initialize()
        dataset_path = self.config.get("dataset_path", "datasets/OpenRCA/Bank")
        default_tz = self.config.get("default_timezone", "Asia/Shanghai")
        self.data_loader = OpenRCADataLoader(dataset_path, default_timezone=default_tz)
        
    def _check_loader(self) -> None:
        """Check if data loader is initialized."""
        if not self.data_loader:
            raise RuntimeError("Tool not initialized. Call initialize() first.")

    # Application Metrics Tools
    
    def get_service_performance(
        self,
        start_time: str,
        end_time: str,
        service_name: Optional[str] = None
    ) -> str:
        self._check_loader()
        df = self.data_loader.load_metrics_for_time_range(start_time, end_time, "app")
        
        if df.empty:
            return f"No application metrics found between {start_time} and {end_time}"
            
        if service_name:
            df = df[df['tc'] == service_name]
            if df.empty:
                return f"No metrics found for service '{service_name}' in the specified time range"
        
        # Calculate statistics
        stats = df.groupby('tc').agg({
            'mrt': ['mean', 'max', 'min'],
            'sr': 'mean',
            'rr': 'mean',
            'cnt': 'sum'
        }).round(2)
        
        # Format output
        output = [f"Service Performance Summary ({start_time} to {end_time}):"]
        
        for service in stats.index:
            s = stats.loc[service]
            output.append(f"\nService: {service}")
            output.append(f"  - Avg Response Time: {s[('mrt', 'mean')]}ms (Range: {s[('mrt', 'min')]}-{s[('mrt', 'max')]}ms)")
            output.append(f"  - Success Rate: {s[('sr', 'mean')]}%")
            output.append(f"  - Request Rate: {s[('rr', 'mean')]}%")
            output.append(f"  - Total Requests: {int(s[('cnt', 'sum')])}")
            
            # Add insights
            if s[('sr', 'mean')] < 99.0:
                output.append(f"  âš ï¸ Low success rate detected (<99%)")
            if s[('mrt', 'mean')] > 500:
                output.append(f"  âš ï¸ High latency detected (>500ms)")
                
        return "\n".join(output)
    
    def get_available_components(
        self,
        start_time: str,
        end_time: str
    ) -> str:
        self._check_loader()
        df = self.data_loader.load_metrics_for_time_range(start_time, end_time, "container")
        
        if df.empty:
            return "No container metrics found in the specified time range"
        
        components = sorted(df['cmdb_id'].unique())
        
        output = [f"Available Components ({len(components)} total):"]
        for comp in components:
            # Get metric count for this component
            metric_count = df[df['cmdb_id'] == comp]['kpi_name'].nunique()
            output.append(f"  - {comp} ({metric_count} metrics)")
        
        return "\n".join(output)
    
    def get_available_metrics(
        self,
        start_time: str,
        end_time: str,
        component_id: Optional[str] = None,
        metric_pattern: Optional[str] = None,
        top: int = 10
    ) -> str:
        self._check_loader()
        df = self.data_loader.load_metrics_for_time_range(start_time, end_time, "container")
        
        if df.empty:
            return "No container metrics found in the specified time range"
        
        if component_id:
            df = df[df['cmdb_id'] == component_id]
            if df.empty:
                return f"No metrics found for component '{component_id}'"
        
        # Apply metric pattern filter if provided
        if metric_pattern:
            df = df[df['kpi_name'].str.contains(metric_pattern, case=False, na=False)]
            if df.empty:
                return f"No metrics matching pattern '{metric_pattern}' found"
        
        metrics = sorted(df['kpi_name'].unique())
        
        # Build header
        if component_id and metric_pattern:
            output = [f"Available Metrics for {component_id} matching '{metric_pattern}' ({len(metrics)} total):"]
        elif component_id:
            output = [f"Available Metrics for {component_id} ({len(metrics)} total):"]
        elif metric_pattern:
            output = [f"Available Metrics matching '{metric_pattern}' ({len(metrics)} total):"]
        else:
            output = [f"Available Metrics (All Components, {len(metrics)} total):"]
        
        # Group metrics by prefix for better readability
        metric_groups = {}
        for metric in metrics:
            # Extract prefix (e.g., "OSLinux-CPU" from "OSLinux-CPU_CPU_CPUUtil")
            parts = metric.split('_')
            if len(parts) > 1:
                prefix = parts[0]
            else:
                prefix = "Other"
            
            if prefix not in metric_groups:
                metric_groups[prefix] = []
            metric_groups[prefix].append(metric)
        
        # Display grouped metrics
        for prefix in sorted(metric_groups.keys()):
            output.append(f"\n{prefix} ({len(metric_groups[prefix])} metrics):")
            for metric in sorted(metric_groups[prefix])[:top]:  # Use top parameter
                output.append(f"  - {metric}")
            if len(metric_groups[prefix]) > top:
                output.append(f"  ... and {len(metric_groups[prefix]) - top} more")
        
        return "\n".join(output)

    def find_slow_services(
        self,
        start_time: str,
        end_time: str,
        threshold_ms: float = 500.0
    ) -> str:
        self._check_loader()
        df = self.data_loader.load_metrics_for_time_range(start_time, end_time, "app")
        
        if df.empty:
            return "No data found for analysis"
            
        # Group by service and calculate mean response time
        avg_rt = df.groupby('tc')['mrt'].mean()
        slow_services = avg_rt[avg_rt > threshold_ms].sort_values(ascending=False)
        
        if slow_services.empty:
            return f"No services exceeded the latency threshold of {threshold_ms}ms"
            
        output = [f"Slow Services Report (Threshold: {threshold_ms}ms):"]
        for service, rt in slow_services.items():
            # Get peak latency for this service
            peak = df[df['tc'] == service]['mrt'].max()
            output.append(f"\nğŸ”´ {service}")
            output.append(f"  - Average Latency: {rt:.2f}ms")
            output.append(f"  - Peak Latency: {peak:.2f}ms")
            
        return "\n".join(output)

    def find_low_success_rate_services(
        self,
        start_time: str,
        end_time: str,
        threshold_percent: float = 95.0
    ) -> str:
        self._check_loader()
        df = self.data_loader.load_metrics_for_time_range(start_time, end_time, "app")
        
        if df.empty:
            return "No data found for analysis"
            
        avg_sr = df.groupby('tc')['sr'].mean()
        problem_services = avg_sr[avg_sr < threshold_percent].sort_values()
        
        if problem_services.empty:
            return f"All services operating above {threshold_percent}% success rate"
            
        output = [f"Low Success Rate Report (Threshold: {threshold_percent}%):"]
        for service, sr in problem_services.items():
            min_sr = df[df['tc'] == service]['sr'].min()
            output.append(f"\nğŸ”´ {service}")
            output.append(f"  - Average Success Rate: {sr:.2f}%")
            output.append(f"  - Minimum Success Rate: {min_sr:.2f}%")
            
        return "\n".join(output)

    # Infrastructure Metrics Tools
    
    def detect_metric_anomalies(
        self,
        start_time: str,
        end_time: str,
        method: str = "both",
        component_id: Optional[str] = None,
        sensitivity: float = 3.0,
        top: int = 10,
        ruptures_algorithm: str = "pelt",
        ruptures_model: str = "rbf",
        pen: float = 5.0,
        z_threshold: Optional[float] = None,
        min_data_points_ruptures: int = 10,
        min_data_points_zscore: int = 5,
        min_consecutive: int = 3
    ) -> str:
        """
        Detect anomalies in core metrics using ruptures or Z-score methods.
        
        This is a robust tool that focuses on core metrics (CPU, memory, disk, network, JVM)
        for candidate components and identifies anomalies with fault start times.
        
        Args:
            start_time: Start time in ISO format
            end_time: End time in ISO format
            method: Detection method - "ruptures", "zscore", or "both" (default: "both")
            component_id: Optional component ID to filter
            sensitivity: Z-score threshold for anomaly detection (default: 3.0)
            top: Maximum number of anomalies to return (default: 10)
            ruptures_algorithm: Algorithm for ruptures - "pelt", "binseg", "dynp", "window" (default: "pelt")
            ruptures_model: Model for ruptures - "rbf", "l1", "l2", "linear", "normal", "ar", "rank" (default: "rbf")
            pen: Penalty parameter for ruptures (default: 5.0)
            z_threshold: Z-score threshold (default: None, uses sensitivity if None)
            min_data_points_ruptures: Minimum data points for ruptures (default: 10)
            min_data_points_zscore: Minimum data points for z-score (default: 5)
            min_consecutive: Minimum consecutive anomalies for z-score (default: 3)
        
        Ruptures Algorithms:
            - "pelt": Pruned Exact Linear Time - Fast and accurate, good for most scenarios (default)
            - "binseg": Binary Segmentation - Fast but may not find global optimum
            - "dynp": Dynamic Programming - Global optimum but computationally expensive
            - "window": Window-based - Good for online detection
        
        Ruptures Models:
            - "rbf": Radial Basis Function - Good for non-linear patterns (default)
            - "l1": L1 norm - Robust to outliers
            - "l2": L2 norm - Standard least squares
            - "linear": Linear model - For linear trends
            - "normal": Normal distribution - For Gaussian data
            - "ar": Auto-regressive - For time series with dependencies
            - "rank": Rank-based - Non-parametric, robust
        """
        self._check_loader()
        
        # å€™é€‰ç»„ä»¶åˆ—è¡¨
        CANDIDATE_COMPONENTS = [
            "apache01", "apache02",
            "Tomcat01", "Tomcat02", "Tomcat03", "Tomcat04",
            "Mysql01", "Mysql02",
            "Redis01", "Redis02",
            "MG01", "MG02",
            "IG01", "IG02"
        ]
        
        # è§„èŒƒåŒ–æ—¶é—´ï¼ˆå¤„ç†æ—¶åŒºï¼‰
        def normalize_time(time_input: str) -> pd.Timestamp:
            try:
                dt = pd.to_datetime(time_input)
                if dt.tzinfo is None:
                    dt = dt.tz_localize('Asia/Shanghai')
                else:
                    dt = dt.tz_convert('Asia/Shanghai')
            except Exception:
                dt = pd.to_datetime(time_input, errors='coerce')
                if pd.isna(dt):
                    raise ValueError(f"æ— æ³•è§£ææ—¶é—´æ ¼å¼: {time_input}")
                if dt.tzinfo is None:
                    dt = dt.tz_localize('Asia/Shanghai')
                else:
                    dt = dt.tz_convert('Asia/Shanghai')
            return dt
        
        start_dt = normalize_time(start_time)
        end_dt = normalize_time(end_time)
        start_time_str = start_dt.strftime('%Y-%m-%dT%H:%M:%S')
        end_time_str = end_dt.strftime('%Y-%m-%dT%H:%M:%S')
        
        # åŠ è½½æ•°æ®
        df = self.data_loader.load_metrics_for_time_range(
            start_time=start_time_str,
            end_time=end_time_str,
            metric_type="container"
        )
        
        if df.empty:
            return json.dumps([], ensure_ascii=False, indent=2)
            
        # ç­›é€‰å€™é€‰ç»„ä»¶
        if component_id:
            components = [component_id]
        else:
            components = CANDIDATE_COMPONENTS
        
        df = df[df['cmdb_id'].isin(components)].copy()
        
        if df.empty:
            return json.dumps([], ensure_ascii=False, indent=2)
        
        # ç­›é€‰æ ¸å¿ƒæŒ‡æ ‡
        def is_core_metric(kpi_name: str) -> tuple[bool, Optional[str]]:
            import re
            kpi_lower = kpi_name.lower()
            if kpi_name == 'OSLinux-CPU_CPU_CPUCpuUtil':
                return True, 'cpu'
            if kpi_name == 'OSLinux-OSLinux_MEMORY_MEMORY_MEMUsedMemPerc':
                return True, 'memory'
            # ç£ç›˜I/Oï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
            # ç£ç›˜è¯»ï¼š".*DSKRead$"ï¼Œç£ç›˜å†™ï¼š".*DSKWrite$"ï¼Œç£ç›˜è¯»å†™ï¼š".*DSKReadWrite$"
            if re.match(r'.*DSKRead$', kpi_name) or re.match(r'.*DSKWrite$', kpi_name) or re.match(r'.*DSKReadWrite$', kpi_name):
                return True, 'disk_io'
            # ç£ç›˜ç©ºé—´ï¼šåŒ…å«diskå’Œspaceæˆ–usageå…³é”®è¯
            if 'disk' in kpi_lower and ('space' in kpi_lower or 'usage' in kpi_lower):
                return True, 'disk_space'
            # ç½‘ç»œæŒ‡æ ‡ï¼šå…³é”®ç½‘ç»œæ€§èƒ½æŒ‡æ ‡
            # ç½‘ç»œå¸¦å®½åˆ©ç”¨ç‡
            if 'NETBandwidthUtil' in kpi_name:
                return True, 'network_bandwidth'
            # ç½‘ç»œé”™è¯¯ï¼šè¾“å…¥/è¾“å‡ºé”™è¯¯
            if re.match(r'.*NETInErr.*$', kpi_name) or re.match(r'.*NETOutErr.*$', kpi_name):
                return True, 'network_error'
            # TCPè¿æ¥æ•°ï¼šæ€»è¿æ¥æ•°å’Œå¼‚å¸¸çŠ¶æ€è¿æ¥
            if 'TotalTcpConnNum' in kpi_name or 'TCP-CLOSE-WAIT' in kpi_name or 'TCP-FIN-WAIT' in kpi_name:
                return True, 'network_connection'
            # å®¹å™¨ç½‘ç»œæµé‡ï¼šæ¥æ”¶å’Œå‘é€å­—èŠ‚æ•°
            if re.match(r'.*NetworkRxBytes$', kpi_name) or re.match(r'.*NetworkTxBytes$', kpi_name):
                return True, 'network_container'
            # JVM CPU Loadï¼šåªåŒ¹é…JVMç›¸å…³çš„CPULoadï¼Œä¸åŒ¹é…ç³»ç»ŸCPU Load
            # ç³»ç»ŸCPU Loadæ˜¯ OSLinux-CPU_CPU_CPULoadï¼Œä¸åº”è¯¥è¢«è¯†åˆ«ä¸ºæ ¸å¿ƒæŒ‡æ ‡
            if 'JVM' in kpi_name and '_CPULoad' in kpi_name:
                return True, 'jvm_cpu'
            if 'HeapMemoryMax' in kpi_name or 'HeapMemoryUsed' in kpi_name:
                return True, 'jvm_oom'
            return False, None
        
        # å¤„ç†JVM OOMæŒ‡æ ‡
        def process_jvm_oom(df: pd.DataFrame) -> pd.DataFrame:
            heap_max_df = df[df['kpi_name'].str.contains('HeapMemoryMax', na=False)].copy()
            heap_used_df = df[df['kpi_name'].str.contains('HeapMemoryUsed', na=False)].copy()
            
            if heap_max_df.empty or heap_used_df.empty:
                return pd.DataFrame()
            
            jvm_oom_data = []
            components_with_heap = set(heap_max_df['cmdb_id'].unique()).intersection(
                set(heap_used_df['cmdb_id'].unique())
            )
            
            for comp in components_with_heap:
                max_data = heap_max_df[heap_max_df['cmdb_id'] == comp].sort_values('datetime')
                used_data = heap_used_df[heap_used_df['cmdb_id'] == comp].sort_values('datetime')
                
                max_data['time_key'] = max_data['datetime'].dt.floor('min')
                used_data['time_key'] = used_data['datetime'].dt.floor('min')
                
                merged = pd.merge(
                    max_data[['time_key', 'value']].rename(columns={'value': 'HeapMemoryMax'}),
                    used_data[['time_key', 'value']].rename(columns={'value': 'HeapMemoryUsed'}),
                    on='time_key',
                    how='inner'
                )
                
                if not merged.empty:
                    merged['HeapUsage'] = merged['HeapMemoryUsed'] / merged['HeapMemoryMax']
                    merged = merged[merged['HeapMemoryMax'] > 0]
                    
                    if not merged.empty:
                        for _, row in merged.iterrows():
                            jvm_oom_data.append({
                                'timestamp': row['time_key'].timestamp(),
                                'cmdb_id': comp,
                                'kpi_name': 'JVM_Heap_Usage',
                                'value': row['HeapUsage'],
                                'datetime': row['time_key']
                            })
            
            if jvm_oom_data:
                return pd.DataFrame(jvm_oom_data)
            return pd.DataFrame()
        
        # ç­›é€‰æ ¸å¿ƒæŒ‡æ ‡
        core_metrics = []
        for kpi in df['kpi_name'].unique():
            is_core, metric_type = is_core_metric(kpi)
            if is_core and metric_type != 'jvm_oom':
                core_metrics.append(df[df['kpi_name'] == kpi])
        
        jvm_oom_df = process_jvm_oom(df)
        if not jvm_oom_df.empty:
            core_metrics.append(jvm_oom_df)
        
        if not core_metrics:
            return json.dumps([], ensure_ascii=False, indent=2)
        
        df_core = pd.concat(core_metrics, ignore_index=True)
        
        # å¤„ç†å‚æ•°
        ruptures_algorithm = ruptures_algorithm.lower()
        ruptures_model = ruptures_model.lower()
        if z_threshold is None:
            z_threshold = sensitivity
        
        # éªŒè¯ç®—æ³•å’Œæ¨¡å‹å‚æ•°
        valid_algorithms = ['pelt', 'binseg', 'dynp', 'window']
        valid_models = ['rbf', 'l1', 'l2', 'linear', 'normal', 'ar', 'rank', 'mahalanobis']
        
        # é»˜è®¤ä½¿ç”¨ Pelt + rbf çš„åŸå› ï¼š
        # - Pelt: å¿«é€Ÿä¸”ç²¾ç¡®ï¼Œçº¿æ€§æ—¶é—´å¤æ‚åº¦ï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯ï¼Œèƒ½æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£
        # - rbf: å¾„å‘åŸºå‡½æ•°æ ¸ï¼Œèƒ½æ•æ‰éçº¿æ€§æ¨¡å¼ï¼Œå¯¹å¤æ‚çš„æ—¶é—´åºåˆ—æ•°æ®è¡¨ç°è‰¯å¥½
        # 
        # å…¶ä»–ç®—æ³•é€‰æ‹©å»ºè®®ï¼š
        # - binseg: å½“æ•°æ®é‡å¾ˆå¤§ä¸”éœ€è¦å¿«é€Ÿæ£€æµ‹æ—¶ä½¿ç”¨ï¼ˆå¯èƒ½ä¸æ˜¯å…¨å±€æœ€ä¼˜ï¼‰
        # - dynp: å½“éœ€è¦ä¿è¯å…¨å±€æœ€ä¼˜ä¸”æ•°æ®é‡ä¸å¤§æ—¶ä½¿ç”¨ï¼ˆè®¡ç®—æˆæœ¬é«˜ï¼‰
        # - window: å½“éœ€è¦åœ¨çº¿æ£€æµ‹æˆ–å®æ—¶ç›‘æ§æ—¶ä½¿ç”¨
        #
        # å…¶ä»–æ¨¡å‹é€‰æ‹©å»ºè®®ï¼š
        # - l1: å½“æ•°æ®åŒ…å«å¼‚å¸¸å€¼æ—¶ä½¿ç”¨ï¼ˆå¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼‰
        # - l2: æ ‡å‡†æœ€å°äºŒä¹˜ï¼Œé€‚åˆçº¿æ€§è¶‹åŠ¿
        # - linear: æ˜ç¡®çŸ¥é“æ•°æ®æ˜¯çº¿æ€§è¶‹åŠ¿æ—¶ä½¿ç”¨
        # - normal: æ•°æ®ç¬¦åˆé«˜æ–¯åˆ†å¸ƒæ—¶ä½¿ç”¨
        # - ar: æ—¶é—´åºåˆ—æœ‰è‡ªç›¸å…³ä¾èµ–æ—¶ä½¿ç”¨
        # - rank: éå‚æ•°æ–¹æ³•ï¼Œå¯¹åˆ†å¸ƒå‡è®¾ä¸æ•æ„Ÿ
        
        if ruptures_algorithm not in valid_algorithms:
            ruptures_algorithm = 'pelt'  # é»˜è®¤ä½¿ç”¨ pelt
        if ruptures_model not in valid_models:
            ruptures_model = 'rbf'  # é»˜è®¤ä½¿ç”¨ rbf
        
        def get_threshold(kpi_name: str) -> float:
            if 'CPU' in kpi_name or 'CPULoad' in kpi_name:
                return 20.0
            elif 'MEM' in kpi_name or 'Memory' in kpi_name or 'Heap_Usage' in kpi_name:
                return 30.0
            elif 'DSK' in kpi_name or 'disk' in kpi_name.lower():
                return 50.0
            elif 'NET' in kpi_name or 'Network' in kpi_name:
                # ç½‘ç»œå¸¦å®½åˆ©ç”¨ç‡ï¼š80%ä»¥ä¸Šéœ€è¦å…³æ³¨
                if 'BandwidthUtil' in kpi_name:
                    return 80.0
                # ç½‘ç»œé”™è¯¯ï¼šä»»ä½•é”™è¯¯éƒ½éœ€è¦å…³æ³¨
                elif 'Err' in kpi_name:
                    return 1.0
                # TCPè¿æ¥æ•°ï¼šå˜åŒ–è¶…è¿‡50%éœ€è¦å…³æ³¨
                elif 'TcpConnNum' in kpi_name or 'TCP-' in kpi_name:
                    return 50.0
                # ç½‘ç»œæµé‡ï¼šå˜åŒ–è¶…è¿‡50%éœ€è¦å…³æ³¨
                else:
                    return 50.0
            else:
                return 30.0
        
        def get_absolute_threshold(kpi_name: str) -> Optional[float]:
            """è·å–ç»å¯¹é˜ˆå€¼ï¼Œç”¨äºæ£€æµ‹æŒç»­é«˜å€¼ï¼ˆå³ä½¿æ²¡æœ‰å˜åŒ–ç‚¹ï¼‰"""
            if 'MEM' in kpi_name or 'Memory' in kpi_name:
                # å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡85%è®¤ä¸ºæ˜¯å¼‚å¸¸
                return 85.0
            elif 'JVM' in kpi_name and 'CPULoad' in kpi_name:
                # JVM CPU Loadè¶…è¿‡20%è®¤ä¸ºæ˜¯å¼‚å¸¸ï¼ˆJVM CPU Loadé€šå¸¸è¾ƒä½ï¼Œ20%å·²ç»å¾ˆé«˜ï¼‰
                return 20.0
            elif 'CPU' in kpi_name or 'CPULoad' in kpi_name:
                # ç³»ç»ŸCPUä½¿ç”¨ç‡è¶…è¿‡80%è®¤ä¸ºæ˜¯å¼‚å¸¸
                return 80.0
            elif 'Heap_Usage' in kpi_name:
                # JVMå †ä½¿ç”¨ç‡è¶…è¿‡90%è®¤ä¸ºæ˜¯å¼‚å¸¸
                return 0.9
            elif 'BandwidthUtil' in kpi_name:
                # ç½‘ç»œå¸¦å®½åˆ©ç”¨ç‡è¶…è¿‡85%è®¤ä¸ºæ˜¯å¼‚å¸¸
                return 85.0
            return None
        
        def get_baseline_params(kpi_name: str) -> tuple[float, float]:
            """
            è·å–åŸºçº¿å‚æ•°ï¼šæœ€å°åŸºçº¿é˜ˆå€¼å’Œå‚è€ƒå€¼
            ç”¨äºå¤„ç†å°åŸºçº¿å€¼çš„æƒ…å†µï¼Œé¿å…é™¤ä»¥æ¥è¿‘0çš„å€¼å¯¼è‡´è¯¯æŠ¥
            
            Returns:
                (min_baseline_threshold, reference_value)
            """
            kpi_lower = kpi_name.lower()
            # CPULoadç±»å‹çš„æŒ‡æ ‡ï¼ˆç»å¯¹å€¼å¾ˆå°ï¼Œé€šå¸¸åœ¨0-1ä¹‹é—´ï¼‰
            if 'cpuload' in kpi_lower:
                return (0.1, 1.0)  # åŸºçº¿é˜ˆå€¼0.1ï¼Œå‚è€ƒå€¼1.0ï¼ˆ100% CPU Loadï¼‰
            # CPUä½¿ç”¨ç‡ï¼ˆç™¾åˆ†æ¯”ï¼Œé€šå¸¸åœ¨0-100ä¹‹é—´ï¼‰
            elif 'cpu' in kpi_lower and 'util' in kpi_lower:
                return (10.0, 100.0)  # åŸºçº¿é˜ˆå€¼10%ï¼Œå‚è€ƒå€¼100%
            # å†…å­˜ä½¿ç”¨ç‡ï¼ˆç™¾åˆ†æ¯”ï¼Œé€šå¸¸åœ¨0-100ä¹‹é—´ï¼‰
            elif 'mem' in kpi_lower or 'memory' in kpi_lower:
                return (10.0, 100.0)  # åŸºçº¿é˜ˆå€¼10%ï¼Œå‚è€ƒå€¼100%
            # JVMå †ä½¿ç”¨ç‡ï¼ˆæ¯”ä¾‹ï¼Œé€šå¸¸åœ¨0-1ä¹‹é—´ï¼‰
            elif 'heap' in kpi_lower:
                return (0.1, 1.0)  # åŸºçº¿é˜ˆå€¼0.1ï¼Œå‚è€ƒå€¼1.0
            # ç½‘ç»œå¸¦å®½åˆ©ç”¨ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
            elif 'bandwidth' in kpi_lower or 'util' in kpi_lower:
                return (10.0, 100.0)  # åŸºçº¿é˜ˆå€¼10%ï¼Œå‚è€ƒå€¼100%
            # é»˜è®¤å€¼ï¼šå¯¹äºå…¶ä»–æŒ‡æ ‡ï¼Œä½¿ç”¨è¾ƒå°çš„é˜ˆå€¼
            else:
                return (0.1, 100.0)  # é»˜è®¤åŸºçº¿é˜ˆå€¼0.1ï¼Œå‚è€ƒå€¼100%
        
        def calculate_severity(deviation_pct: float, max_value: float) -> str:
            if deviation_pct > 100:
                severity = "ä¸¥é‡"
            elif deviation_pct > 50:
                severity = "æ˜¾è‘—"
            else:
                severity = "ä¸­ç­‰"
            return f"{severity}ï¼ˆæœ€å¤§å€¼ï¼š{max_value:.1f}ï¼Œåç¦»ï¼š{deviation_pct:.1f}%ï¼‰"
        
        # ä½¿ç”¨rupturesæ£€æµ‹
        def detect_with_ruptures(component: str, kpi_name: str, data: pd.DataFrame) -> List[Dict[str, Any]]:
            if not RUPTURES_AVAILABLE or len(data) < min_data_points_ruptures:
                return []
            
            data = data.sort_values('datetime').reset_index(drop=True)
            values = data['value'].values.astype(float)
            # ç›´æ¥ä»DataFrameè·å–datetimeåˆ—ï¼Œä¿ç•™æ—¶åŒºä¿¡æ¯
            datetime_series = data['datetime']
            
            anomalies = []
            try:
                signal = values.reshape(-1, 1)
                
                # æ ¹æ®é€‰æ‹©çš„ç®—æ³•åˆ›å»ºæ£€æµ‹å™¨
                if ruptures_algorithm == 'pelt':
                    algo = rpt.Pelt(model=ruptures_model).fit(signal)
                    change_points = algo.predict(pen=pen)
                elif ruptures_algorithm == 'binseg':
                    algo = rpt.Binseg(model=ruptures_model).fit(signal)
                    # Binsegå¯ä»¥ä½¿ç”¨penaltyæˆ–n_bkpså‚æ•°
                    # ä¼˜å…ˆä½¿ç”¨penaltyå‚æ•°ï¼Œå¦‚æœpenaltyæ— æ•ˆåˆ™ä½¿ç”¨n_bkps
                    max_n_bkps = max(2, min(10, len(values) // 10))
                    try:
                        change_points = algo.predict(pen=pen)
                    except (TypeError, ValueError):
                        # å¦‚æœpenaltyå‚æ•°ä¸æ”¯æŒï¼Œä½¿ç”¨n_bkps
                        change_points = algo.predict(n_bkps=max_n_bkps)
                elif ruptures_algorithm == 'dynp':
                    algo = rpt.Dynp(model=ruptures_model).fit(signal)
                    # Dynpå¿…é¡»æŒ‡å®šæœ€å¤§å˜åŒ–ç‚¹æ•°
                    max_n_bkps = max(2, min(10, len(values) // 10))
                    change_points = algo.predict(n_bkps=max_n_bkps)
                elif ruptures_algorithm == 'window':
                    algo = rpt.Window(width=min(40, len(values) // 2), model=ruptures_model).fit(signal)
                    change_points = algo.predict(pen=pen)
                else:
                    # é»˜è®¤ä½¿ç”¨ Pelt
                    algo = rpt.Pelt(model=ruptures_model).fit(signal)
                    change_points = algo.predict(pen=pen)
                
                if len(change_points) > 1 and change_points[-1] == len(values):
                    change_points = change_points[:-1]
                
                if len(change_points) == 0:
                    return []
                
                # åˆ†ææ®µ
                segments = []
                prev_cp = 0
                for cp in change_points:
                    if cp > prev_cp and cp <= len(values):
                        segment_values = values[prev_cp:cp]
                        if len(segment_values) > 0:
                            segments.append({
                                'start_idx': prev_cp,
                                'end_idx': cp - 1,
                                'mean': np.mean(segment_values),
                                'max': np.max(segment_values),
                                'length': len(segment_values)
                            })
                        prev_cp = cp
                
                if prev_cp < len(values):
                    segment_values = values[prev_cp:]
                    if len(segment_values) > 0:
                        segments.append({
                            'start_idx': prev_cp,
                            'end_idx': len(values) - 1,
                            'mean': np.mean(segment_values),
                            'max': np.max(segment_values),
                            'length': len(segment_values)
                        })
                
                # è¯†åˆ«å‰§çƒˆå˜åŒ– - åŸºäºç›¸å¯¹å˜åŒ–è€Œéå›ºå®šé˜ˆå€¼
                if len(segments) >= 2:
                    # è®¡ç®—æ•´ä¸ªæ—¶é—´åºåˆ—çš„åŸºçº¿ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºåŠ¨æ€é˜ˆå€¼ï¼‰
                    overall_mean = np.mean(values)
                    overall_std = np.std(values)
                    baseline_value = overall_mean
                    
                    # å¦‚æœæ ‡å‡†å·®å¾ˆå°ï¼Œä½¿ç”¨å‡å€¼ä½œä¸ºåŸºçº¿ï¼›å¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªsegmentçš„å‡å€¼ä½œä¸ºåŸºçº¿
                    if overall_std < overall_mean * 0.1:  # æ ‡å‡†å·®å°äºå‡å€¼çš„10%ï¼Œè®¤ä¸ºæ•°æ®ç›¸å¯¹ç¨³å®š
                        baseline_value = overall_mean
                    else:
                        # ä½¿ç”¨å‰å‡ ä¸ªsegmentsçš„å‡å€¼ä½œä¸ºåŸºçº¿ï¼ˆæ’é™¤å¯èƒ½çš„å¼‚å¸¸æ®µï¼‰
                        baseline_segments = segments[:min(3, len(segments))]
                        baseline_values = [seg['mean'] for seg in baseline_segments]
                        baseline_value = np.mean(baseline_values)
                    
                    for i in range(1, len(segments)):
                        prev_seg = segments[i-1]
                        curr_seg = segments[i]
                        
                        # è·å–åŸºçº¿å‚æ•°ï¼ˆæ ¹æ®æŒ‡æ ‡ç±»å‹åŠ¨æ€è®¾ç½®ï¼‰
                        min_baseline_threshold, reference_value = get_baseline_params(kpi_name)
                        
                        # è®¡ç®—ç›¸å¯¹å˜åŒ–ç™¾åˆ†æ¯”ï¼ˆæ”¹è¿›ç‰ˆï¼šå¤„ç†å°åŸºçº¿å€¼çš„æƒ…å†µï¼‰
                        if prev_seg['mean'] >= min_baseline_threshold:
                            relative_change_pct = abs((curr_seg['mean'] - prev_seg['mean']) / prev_seg['mean'] * 100)
                        else:
                            # å‰ä¸€ä¸ªsegmentçš„å‡å€¼å¾ˆå°ï¼Œä½¿ç”¨ç»å¯¹å˜åŒ–
                            absolute_change = abs(curr_seg['mean'] - prev_seg['mean'])
                            relative_change_pct = (absolute_change / reference_value) * 100 if reference_value > 0 else 0
                        
                        # è®¡ç®—ç›¸å¯¹äºåŸºçº¿çš„åç¦»ç™¾åˆ†æ¯”ï¼ˆæ”¹è¿›ç‰ˆï¼šå¤„ç†å°åŸºçº¿å€¼çš„æƒ…å†µï¼‰
                        if baseline_value >= min_baseline_threshold:
                            baseline_deviation_pct = abs((curr_seg['mean'] - baseline_value) / baseline_value * 100)
                        else:
                            # åŸºçº¿å€¼å¾ˆå°ï¼Œä½¿ç”¨ç»å¯¹å˜åŒ–
                            absolute_change = abs(curr_seg['mean'] - baseline_value)
                            baseline_deviation_pct = (absolute_change / reference_value) * 100 if reference_value > 0 else 0
                        
                        # åŠ¨æ€é˜ˆå€¼ï¼šåŸºäºæ—¶é—´çª—å£å†…çš„ç»Ÿè®¡ç‰¹æ€§
                        # 1. ç›¸å¯¹å˜åŒ–é˜ˆå€¼ï¼šsegmentä¹‹é—´çš„ç›¸å¯¹å˜åŒ–
                        # 2. åŸºçº¿åç¦»é˜ˆå€¼ï¼šç›¸å¯¹äºåŸºçº¿çš„åç¦»
                        # 3. æ ‡å‡†å·®å€æ•°ï¼šå¦‚æœå˜åŒ–è¶…è¿‡å¤šä¸ªæ ‡å‡†å·®ï¼Œè®¤ä¸ºæ˜¯å¼‚å¸¸
                        
                        # è®¡ç®—å˜åŒ–çš„æ ‡å‡†å·®å€æ•°
                        if overall_std > 0:
                            change_in_std = abs(curr_seg['mean'] - prev_seg['mean']) / overall_std
                        else:
                            change_in_std = 0
                        
                        # åŠ¨æ€åˆ¤æ–­æ˜¯å¦ä¸ºå¼‚å¸¸ï¼š
                        # 1. ç›¸å¯¹å˜åŒ– > 50%ï¼ˆsegmentä¹‹é—´å˜åŒ–è¶…è¿‡50%ï¼‰
                        # 2. æˆ–è€…ç›¸å¯¹å˜åŒ– > 30% ä¸” åŸºçº¿åç¦» > 50%ï¼ˆå˜åŒ–æ˜æ˜¾ä¸”åç¦»åŸºçº¿ï¼‰
                        # 3. æˆ–è€…å˜åŒ–è¶…è¿‡2ä¸ªæ ‡å‡†å·®ï¼ˆç»Ÿè®¡æ˜¾è‘—ï¼‰
                        # 4. æˆ–è€…ç›¸å¯¹å˜åŒ– > 100%ï¼ˆå˜åŒ–è¶…è¿‡ä¸€å€ï¼‰
                        
                        is_anomaly = False
                        if relative_change_pct > 100:
                            # å˜åŒ–è¶…è¿‡ä¸€å€ï¼Œè‚¯å®šæ˜¯å¼‚å¸¸
                            is_anomaly = True
                        elif relative_change_pct > 50:
                            # å˜åŒ–è¶…è¿‡50%ï¼Œè®¤ä¸ºæ˜¯å¼‚å¸¸
                            is_anomaly = True
                        elif relative_change_pct > 30 and baseline_deviation_pct > 50:
                            # å˜åŒ–è¶…è¿‡30%ä¸”åç¦»åŸºçº¿è¶…è¿‡50%
                            is_anomaly = True
                        elif change_in_std > 2.0:
                            # å˜åŒ–è¶…è¿‡2ä¸ªæ ‡å‡†å·®ï¼Œç»Ÿè®¡æ˜¾è‘—
                            is_anomaly = True
                        
                            if is_anomaly:
                                # ä½¿ç”¨ç›¸å¯¹å˜åŒ–ç™¾åˆ†æ¯”å’ŒåŸºçº¿åç¦»ç™¾åˆ†æ¯”ä¸­çš„è¾ƒå¤§å€¼
                                deviation_pct = max(relative_change_pct, baseline_deviation_pct)
                                
                                change_idx = curr_seg['start_idx']
                                if change_idx < len(datetime_series):
                                    # ç›´æ¥ä»DataFrameè·å–æ—¶é—´æˆ³ï¼Œä¿ç•™æ—¶åŒºä¿¡æ¯
                                    change_time = datetime_series.iloc[change_idx]
                            anomalies.append({
                                'component_name': component,
                                'faulty_kpi': kpi_name,
                                'fault_start_time': to_iso_shanghai(change_time),
                                'severity_score': calculate_severity(deviation_pct, curr_seg['max']),
                                'deviation_pct': float(deviation_pct),  # ç¡®ä¿æ˜¯Python floatç±»å‹
                                'method': 'ruptures',
                                'change_idx': int(change_idx)  # ç¡®ä¿æ˜¯Python intç±»å‹
                            })
            except Exception:
                pass
            
            return anomalies
        
        # ä½¿ç”¨Z-scoreæ£€æµ‹ - æ”¹è¿›ç‰ˆï¼šåŸºäºæ»‘åŠ¨çª—å£åŸºçº¿
        def detect_with_zscore(component: str, kpi_name: str, data: pd.DataFrame) -> List[Dict[str, Any]]:
            if len(data) < min_data_points_zscore:
                return []
            
            data = data.sort_values('datetime').reset_index(drop=True)
            values = data['value'].values.astype(float)
            # ç›´æ¥ä»DataFrameè·å–datetimeåˆ—ï¼Œä¿ç•™æ—¶åŒºä¿¡æ¯
            datetime_series = data['datetime']
            
            anomalies = []
            try:
                # æ–¹æ³•1ï¼šå…¨å±€Z-scoreæ£€æµ‹ï¼ˆåŸæœ‰æ–¹æ³•ï¼‰
                mean_val = np.mean(values)
                std_val = np.std(values)
                
                if std_val == 0:
                    return []
                
                z_scores = np.abs((values - mean_val) / std_val)
                anomaly_indices_global = np.where(z_scores > z_threshold)[0]
                
                # æ–¹æ³•2ï¼šæ»‘åŠ¨çª—å£åŸºçº¿æ£€æµ‹ï¼ˆæ”¹è¿›ç‰ˆï¼šé¿å…åŸºçº¿çª—å£æ±¡æŸ“ï¼‰
                # ä½¿ç”¨å‰10-15%çš„æ•°æ®ä½œä¸ºåŸºçº¿çª—å£ï¼ˆæ›´å°çš„çª—å£ï¼Œå‡å°‘å¼‚å¸¸å€¼æ±¡æŸ“ï¼‰
                baseline_window_size = max(5, min(int(len(values) * 0.15), int(len(values) * 0.1)))
                baseline_values_raw = values[:baseline_window_size]
                
                # ä½¿ç”¨IQRæ–¹æ³•æ’é™¤åŸºçº¿çª—å£ä¸­çš„å¼‚å¸¸å€¼
                if len(baseline_values_raw) >= 4:
                    q1 = np.percentile(baseline_values_raw, 25)
                    q3 = np.percentile(baseline_values_raw, 75)
                    iqr = q3 - q1
                    if iqr > 0:
                        # æ’é™¤è¶…å‡º1.5*IQRèŒƒå›´çš„å¼‚å¸¸å€¼
                        lower_bound = q1 - 1.5 * iqr
                        upper_bound = q3 + 1.5 * iqr
                        baseline_values = baseline_values_raw[
                            (baseline_values_raw >= lower_bound) & (baseline_values_raw <= upper_bound)
                        ]
                    else:
                        baseline_values = baseline_values_raw
                else:
                    baseline_values = baseline_values_raw
                
                # å¦‚æœæ’é™¤å¼‚å¸¸å€¼ååŸºçº¿çª—å£å¤ªå°ï¼Œä½¿ç”¨åŸå§‹å€¼
                if len(baseline_values) < 3:
                    baseline_values = baseline_values_raw
                
                baseline_mean = np.mean(baseline_values)
                baseline_std = np.std(baseline_values)
                
                # å¦‚æœåŸºçº¿æ ‡å‡†å·®ä¸º0æˆ–å¤ªå°ï¼Œä½¿ç”¨MADï¼ˆMedian Absolute Deviationï¼‰ä½œä¸ºæ›¿ä»£
                if baseline_std == 0 or baseline_std < baseline_mean * 0.01:
                    # ä½¿ç”¨MADï¼šmedian(|x - median(x)|)
                    baseline_median = np.median(baseline_values)
                    mad = np.median(np.abs(baseline_values - baseline_median))
                    # MADçš„æ ‡å‡†åŒ–ï¼šå¯¹äºæ­£æ€åˆ†å¸ƒï¼ŒMAD â‰ˆ 0.6745 * stdï¼Œæ‰€ä»¥ std â‰ˆ MAD / 0.6745
                    baseline_std = mad / 0.6745 if mad > 0 else std_val
                
                if baseline_std == 0:
                    baseline_std = std_val  # å¦‚æœåŸºçº¿æ ‡å‡†å·®ä»ä¸º0ï¼Œä½¿ç”¨å…¨å±€æ ‡å‡†å·®
                
                # è®¡ç®—ç›¸å¯¹äºåŸºçº¿çš„Z-score
                baseline_z_scores = np.abs((values - baseline_mean) / baseline_std) if baseline_std > 0 else np.zeros_like(values)
                anomaly_indices_baseline = np.where(baseline_z_scores > z_threshold)[0]
                
                # åˆå¹¶ä¸¤ç§æ–¹æ³•çš„å¼‚å¸¸ç´¢å¼•
                anomaly_indices = np.unique(np.concatenate([anomaly_indices_global, anomaly_indices_baseline]))
                
                if len(anomaly_indices) == 0:
                    return []
                
                # è¿ç»­æ€§æ£€æŸ¥
                continuous_segments = []
                current_segment = []
                
                for idx in sorted(anomaly_indices):
                    if not current_segment:
                        current_segment.append(idx)
                    elif idx == current_segment[-1] + 1:
                        current_segment.append(idx)
                    else:
                        if len(current_segment) >= min_consecutive:
                            continuous_segments.append(current_segment.copy())
                        current_segment = [idx]
                
                if len(current_segment) >= min_consecutive:
                    continuous_segments.append(current_segment)
                
                # åˆ†ææ¯ä¸ªè¿ç»­å¼‚å¸¸æ®µ
                for segment in continuous_segments:
                    if len(segment) > 0:
                        segment_values = values[segment]
                        
                        segment_mean = np.mean(segment_values)
                        
                        # è·å–åŸºçº¿å‚æ•°ï¼ˆæ ¹æ®æŒ‡æ ‡ç±»å‹åŠ¨æ€è®¾ç½®ï¼‰
                        min_baseline_threshold, reference_value = get_baseline_params(kpi_name)
                        
                        # è®¡ç®—ç›¸å¯¹äºåŸºçº¿çš„åç¦»ç™¾åˆ†æ¯”ï¼ˆæ”¹è¿›ç‰ˆï¼šå¤„ç†å°åŸºçº¿å€¼çš„æƒ…å†µï¼‰
                        # å¯¹äºç»å¯¹å€¼å¾ˆå°çš„æŒ‡æ ‡ï¼ˆå¦‚CPULoadï¼‰ï¼Œå½“åŸºçº¿å€¼å¾ˆå°æ—¶ï¼Œä½¿ç”¨ç»å¯¹å˜åŒ–è€Œä¸æ˜¯ç›¸å¯¹å˜åŒ–
                        # è®¾ç½®æœ€å°åŸºçº¿å€¼é˜ˆå€¼ï¼Œé¿å…é™¤ä»¥æ¥è¿‘0çš„å€¼å¯¼è‡´è¯¯æŠ¥
                        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨çš„baseline_meanæ˜¯æ”¹è¿›åçš„åŸºçº¿å‡å€¼ï¼ˆå·²æ’é™¤å¼‚å¸¸å€¼ï¼‰
                        if baseline_mean >= min_baseline_threshold:
                            # åŸºçº¿å€¼è¶³å¤Ÿå¤§ï¼Œä½¿ç”¨ç›¸å¯¹ç™¾åˆ†æ¯”
                            deviation_pct = abs((segment_mean - baseline_mean) / baseline_mean * 100)
                        elif mean_val >= min_baseline_threshold:
                            # å¦‚æœåŸºçº¿å‡å€¼å¤ªå°ï¼Œä½¿ç”¨å…¨å±€å‡å€¼ï¼ˆå¦‚æœå…¨å±€å‡å€¼ä¹Ÿè¶³å¤Ÿå¤§ï¼‰
                            deviation_pct = abs((segment_mean - mean_val) / mean_val * 100)
                        else:
                            # åŸºçº¿å€¼å’Œå…¨å±€å‡å€¼éƒ½å¾ˆå°ï¼Œä½¿ç”¨ç»å¯¹å˜åŒ–
                            # å°†ç»å¯¹å˜åŒ–è½¬æ¢ä¸ºç­‰æ•ˆçš„ç™¾åˆ†æ¯”ï¼ˆåŸºäºæŒ‡æ ‡ç±»å‹çš„å‚è€ƒå€¼ï¼‰
                            absolute_change = abs(segment_mean - baseline_mean)
                            deviation_pct = (absolute_change / reference_value) * 100 if reference_value > 0 else 0
                        
                        # åŠ¨æ€é˜ˆå€¼ï¼šåŸºäºç›¸å¯¹å˜åŒ–è€Œéå›ºå®šé˜ˆå€¼
                        # 1. åç¦»åŸºçº¿è¶…è¿‡50%ï¼ˆç›¸å¯¹å˜åŒ–æ˜æ˜¾ï¼‰
                        # 2. æˆ–è€…åç¦»åŸºçº¿è¶…è¿‡30%ä¸”Z-scoreå¾ˆé«˜ï¼ˆç»Ÿè®¡æ˜¾è‘—ï¼‰
                        # 3. æˆ–è€…åç¦»åŸºçº¿è¶…è¿‡100%ï¼ˆå˜åŒ–è¶…è¿‡ä¸€å€ï¼‰
                        
                        segment_z_score = np.mean(baseline_z_scores[segment]) if len(segment) > 0 else 0
                        
                        is_anomaly = False
                        if deviation_pct > 100:
                            # å˜åŒ–è¶…è¿‡ä¸€å€ï¼Œè‚¯å®šæ˜¯å¼‚å¸¸
                            is_anomaly = True
                        elif deviation_pct > 50:
                            # åç¦»åŸºçº¿è¶…è¿‡50%ï¼Œè®¤ä¸ºæ˜¯å¼‚å¸¸
                            is_anomaly = True
                        elif deviation_pct > 30 and segment_z_score > z_threshold:
                            # åç¦»åŸºçº¿è¶…è¿‡30%ä¸”Z-scoreè¶…è¿‡é˜ˆå€¼
                            is_anomaly = True
                        
                        if is_anomaly:
                            max_value = np.max(segment_values)
                            # ç›´æ¥ä»DataFrameè·å–æ—¶é—´æˆ³ï¼Œä¿ç•™æ—¶åŒºä¿¡æ¯
                            segment_start_idx = segment[0]
                            if segment_start_idx < len(datetime_series):
                                segment_time = datetime_series.iloc[segment_start_idx]
                            else:
                                segment_time = datetime_series.iloc[0]  # fallback
                            
                            anomalies.append({
                                'component_name': component,
                                'faulty_kpi': kpi_name,
                                'fault_start_time': to_iso_shanghai(segment_time),
                                'severity_score': calculate_severity(deviation_pct, max_value),
                                'deviation_pct': float(deviation_pct),  # ç¡®ä¿æ˜¯Python floatç±»å‹
                                'method': 'zscore',
                                'change_idx': int(segment[0])  # ç¡®ä¿æ˜¯Python intç±»å‹
                            })
            except Exception:
                pass
            
            return anomalies
        
        # åŸºäºç»å¯¹é˜ˆå€¼çš„æ£€æµ‹ï¼ˆç”¨äºæ£€æµ‹æŒç»­é«˜å€¼ï¼‰
        def detect_with_absolute_threshold(component: str, kpi_name: str, data: pd.DataFrame) -> List[Dict[str, Any]]:
            """
            æ£€æµ‹æŒç»­é«˜å€¼ï¼Œå³ä½¿æ²¡æœ‰å˜åŒ–ç‚¹
            
            æ”¹è¿›é€»è¾‘ï¼š
            1. ä½¿ç”¨åŸºçº¿çª—å£ï¼ˆå‰30%æ•°æ®ï¼‰æ¥åˆ¤æ–­æ˜¯å¦ä¸ºæ­£å¸¸çŠ¶æ€
            2. å¦‚æœåŸºçº¿çª—å£ä¹Ÿæ˜¯é«˜å€¼ä¸”æ•´ä¸ªæ—¶é—´çª—å£ç¨³å®šï¼Œå¯èƒ½æ˜¯æ­£å¸¸çŠ¶æ€ï¼Œä¸æŠ¥å‘Šå¼‚å¸¸
            3. åªæœ‰å½“ä»ä½å€¼å˜åŒ–åˆ°é«˜å€¼ï¼Œæˆ–è€…åŸºçº¿çª—å£ä½ä½†åç»­æœ‰é«˜å€¼æ®µæ—¶ï¼Œæ‰æŠ¥å‘Šå¼‚å¸¸
            """
            abs_threshold = get_absolute_threshold(kpi_name)
            if abs_threshold is None:
                return []
            
            if len(data) < min_data_points_zscore:
                return []
            
            data = data.sort_values('datetime').reset_index(drop=True)
            values = data['value'].values.astype(float)
            # ç›´æ¥ä»DataFrameè·å–datetimeåˆ—ï¼Œä¿ç•™æ—¶åŒºä¿¡æ¯
            datetime_series = data['datetime']
            
            # ä½¿ç”¨å‰10-15%çš„æ•°æ®ä½œä¸ºåŸºçº¿çª—å£ï¼ˆæ›´å°çš„çª—å£ï¼Œå‡å°‘å¼‚å¸¸å€¼æ±¡æŸ“ï¼‰
            baseline_window_size = max(5, min(int(len(values) * 0.15), int(len(values) * 0.1)))
            baseline_values_raw = values[:baseline_window_size]
            
            # ä½¿ç”¨IQRæ–¹æ³•æ’é™¤åŸºçº¿çª—å£ä¸­çš„å¼‚å¸¸å€¼
            if len(baseline_values_raw) >= 4:
                q1 = np.percentile(baseline_values_raw, 25)
                q3 = np.percentile(baseline_values_raw, 75)
                iqr = q3 - q1
                if iqr > 0:
                    # æ’é™¤è¶…å‡º1.5*IQRèŒƒå›´çš„å¼‚å¸¸å€¼
                    lower_bound = q1 - 1.5 * iqr
                    upper_bound = q3 + 1.5 * iqr
                    baseline_values = baseline_values_raw[
                        (baseline_values_raw >= lower_bound) & (baseline_values_raw <= upper_bound)
                    ]
                else:
                    baseline_values = baseline_values_raw
            else:
                baseline_values = baseline_values_raw
            
            # å¦‚æœæ’é™¤å¼‚å¸¸å€¼ååŸºçº¿çª—å£å¤ªå°ï¼Œä½¿ç”¨åŸå§‹å€¼
            if len(baseline_values) < 3:
                baseline_values = baseline_values_raw
            
            baseline_mean = np.mean(baseline_values)
            baseline_std = np.std(baseline_values)
            
            # è®¡ç®—æ•´ä¸ªæ—¶é—´çª—å£çš„ç»Ÿè®¡ä¿¡æ¯
            overall_mean = np.mean(values)
            overall_std = np.std(values)
            
            # æ”¹è¿›1ï¼šå¦‚æœåŸºçº¿çª—å£ä¹Ÿæ˜¯é«˜å€¼ï¼Œä¸”æ•´ä¸ªæ—¶é—´çª—å£ç¨³å®šï¼ˆæ ‡å‡†å·®å°ï¼‰ï¼Œå¯èƒ½æ˜¯æ­£å¸¸çŠ¶æ€
            # åˆ¤æ–­æ˜¯å¦ä¸ºç¨³å®šçŠ¶æ€ï¼šæ ‡å‡†å·®å°äºå‡å€¼çš„5%æˆ–å°äºé˜ˆå€¼çš„5%
            stability_threshold = min(overall_mean * 0.05, abs_threshold * 0.05)
            is_stable = overall_std < stability_threshold
            
            # å¦‚æœåŸºçº¿çª—å£å¹³å‡å€¼ä¹Ÿè¶…è¿‡é˜ˆå€¼ï¼Œä¸”æ•´ä¸ªæ—¶é—´çª—å£ç¨³å®šï¼Œå¯èƒ½æ˜¯æ­£å¸¸çŠ¶æ€
            if baseline_mean > abs_threshold and is_stable:
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ•°æ®ç‚¹éƒ½è¶…è¿‡é˜ˆå€¼
                if np.all(values > abs_threshold):
                    # æ•´ä¸ªæ—¶é—´çª—å£éƒ½æ˜¯é«˜å€¼ä¸”ç¨³å®šï¼Œå¯èƒ½æ˜¯ç»„ä»¶çš„æ­£å¸¸çŠ¶æ€ï¼Œä¸æŠ¥å‘Šå¼‚å¸¸
                    return []
            
            # æ”¹è¿›2ï¼šå¯¹äºCPUç­‰å¿«é€Ÿå˜åŒ–çš„æŒ‡æ ‡ï¼Œå…è®¸æ¥è¿‘é˜ˆå€¼çš„æ•°æ®ç‚¹ä¹Ÿç®—ä½œå¼‚å¸¸æ®µçš„ä¸€éƒ¨åˆ†
            # å¯¹äºCPUæŒ‡æ ‡ï¼Œä½¿ç”¨"è½¯é˜ˆå€¼"ï¼šé˜ˆå€¼*0.875ï¼ˆå³80%é˜ˆå€¼çš„87.5% = 70%ï¼‰
            # è¿™æ ·å¯ä»¥æ•è·71.37%è¿™æ ·çš„æ¥è¿‘é˜ˆå€¼çš„é«˜å€¼
            is_cpu_metric = 'CPU' in kpi_name or 'CPULoad' in kpi_name
            # å¯¹äºCPUæŒ‡æ ‡ï¼Œä½¿ç”¨æ›´ä½çš„è½¯é˜ˆå€¼ï¼ˆ70%ï¼‰ï¼Œä»¥ä¾¿æ•è·æ¥è¿‘é˜ˆå€¼çš„é«˜å€¼
            soft_threshold = abs_threshold * 0.875 if is_cpu_metric else abs_threshold
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å€¼è¶…è¿‡ç»å¯¹é˜ˆå€¼æˆ–è½¯é˜ˆå€¼
            high_value_indices = np.where(values > abs_threshold)[0]
            near_threshold_indices = np.where((values > soft_threshold) & (values <= abs_threshold))[0] if is_cpu_metric else np.array([])
            
            # åˆå¹¶è¶…è¿‡é˜ˆå€¼å’Œæ¥è¿‘é˜ˆå€¼çš„æ•°æ®ç‚¹
            if len(near_threshold_indices) > 0:
                all_high_indices = np.unique(np.concatenate([high_value_indices, near_threshold_indices]))
            else:
                all_high_indices = high_value_indices
            
            if len(all_high_indices) == 0:
                return []
            
            # æ”¹è¿›3ï¼šå¯¹äºCPUæŒ‡æ ‡ï¼Œé™ä½min_consecutiveè¦æ±‚ï¼ˆä»3é™åˆ°1ï¼‰
            # å› ä¸ºCPUå¯èƒ½å¿«é€Ÿå˜åŒ–ï¼Œåªæœ‰1-2ä¸ªç‚¹è¶…è¿‡é˜ˆå€¼
            # å¦‚æœæœ‰ä¸€ä¸ªç‚¹è¶…è¿‡ç»å¯¹é˜ˆå€¼ï¼Œå³ä½¿å‰åæ²¡æœ‰æ¥è¿‘é˜ˆå€¼çš„é«˜å€¼ç‚¹ï¼Œä¹Ÿåº”è¯¥è®¤ä¸ºæ˜¯å¼‚å¸¸
            effective_min_consecutive = 1 if is_cpu_metric else min_consecutive
            
            # æ”¹è¿›4ï¼šåªæœ‰å½“ä»ä½å€¼å˜åŒ–åˆ°é«˜å€¼æ—¶ï¼Œæ‰æŠ¥å‘Šå¼‚å¸¸
            # å¦‚æœåŸºçº¿çª—å£ä½äºé˜ˆå€¼ï¼Œä½†åç»­æœ‰é«˜å€¼æ®µï¼Œåˆ™æŠ¥å‘Šå¼‚å¸¸
            # æˆ–è€…ï¼Œå¦‚æœåŸºçº¿çª—å£é«˜äºé˜ˆå€¼ï¼Œä½†åç»­æœ‰æ›´é«˜çš„å€¼æ®µï¼ˆæ˜¾è‘—ä¸Šå‡ï¼‰ï¼Œä¹ŸæŠ¥å‘Šå¼‚å¸¸
            
            # æ‰¾åˆ°è¿ç»­çš„é«˜å€¼æ®µï¼ˆåŒ…æ‹¬æ¥è¿‘é˜ˆå€¼çš„æ•°æ®ç‚¹ï¼‰
            continuous_segments = []
            current_segment = []
            
            for idx in sorted(all_high_indices):
                if not current_segment:
                    current_segment.append(idx)
                elif idx == current_segment[-1] + 1:
                    current_segment.append(idx)
                else:
                    if len(current_segment) >= effective_min_consecutive:
                        continuous_segments.append(current_segment.copy())
                    current_segment = [idx]
            
            if len(current_segment) >= effective_min_consecutive:
                continuous_segments.append(current_segment)
            
            anomalies = []
            for segment in continuous_segments:
                if len(segment) > 0:
                    segment_values = values[segment]
                    segment_mean = np.mean(segment_values)
                    max_value = np.max(segment_values)
                    
                    # æ”¹è¿›3ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ä»ä½å€¼å˜åŒ–åˆ°é«˜å€¼
                    # å¦‚æœsegmentå¼€å§‹å‰æœ‰æ•°æ®ç‚¹ï¼Œæ£€æŸ¥å‰ä¸€ä¸ªç‚¹çš„å€¼
                    segment_start_idx = segment[0]
                    if segment_start_idx > 0:
                        prev_value = values[segment_start_idx - 1]
                        # å¦‚æœå‰ä¸€ä¸ªå€¼ä½äºé˜ˆå€¼ï¼Œè¯´æ˜æ˜¯ä»ä½å€¼å˜åŒ–åˆ°é«˜å€¼ï¼Œè¿™æ˜¯å¼‚å¸¸
                        is_change_from_low = prev_value <= abs_threshold
                    else:
                        # segmentä»å¼€å§‹å°±æœ‰ï¼Œæ£€æŸ¥åŸºçº¿çª—å£
                        is_change_from_low = baseline_mean <= abs_threshold
                    
                    # æ”¹è¿›4ï¼šå¦‚æœåŸºçº¿çª—å£ä¹Ÿæ˜¯é«˜å€¼ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—ä¸Šå‡
                    # å¦‚æœsegmentå‡å€¼æ˜¾è‘—é«˜äºåŸºçº¿å‡å€¼ï¼ˆè¶…è¿‡10%ï¼‰ï¼Œä¹Ÿè®¤ä¸ºæ˜¯å¼‚å¸¸
                    if baseline_mean > abs_threshold:
                        relative_increase = (segment_mean - baseline_mean) / baseline_mean * 100
                        is_significant_increase = relative_increase > 10.0
                    else:
                        is_significant_increase = False
                    
                    # åªæœ‰å½“ä»ä½å€¼å˜åŒ–åˆ°é«˜å€¼ï¼Œæˆ–è€…æœ‰æ˜¾è‘—ä¸Šå‡æ—¶ï¼Œæ‰æŠ¥å‘Šå¼‚å¸¸
                    if not (is_change_from_low or is_significant_increase):
                        # å¦‚æœåŸºçº¿çª—å£ä¹Ÿæ˜¯é«˜å€¼ï¼Œä¸”æ²¡æœ‰æ˜¾è‘—å˜åŒ–ï¼Œå¯èƒ½æ˜¯æ­£å¸¸çŠ¶æ€ï¼Œè·³è¿‡
                        continue
                    
                    # è®¡ç®—æ­£å¸¸åŸºçº¿å€¼ï¼ˆç”¨äºè®¡ç®—åç¦»ç™¾åˆ†æ¯”ï¼‰
                    # ä¼˜å…ˆä½¿ç”¨segmentå¼€å§‹å‰çš„æ­£å¸¸å€¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åŸºçº¿çª—å£ä¸­ä½äºé˜ˆå€¼çš„æ•°æ®ç‚¹
                    normal_baseline = None
                    
                    # æ–¹æ³•1ï¼šä½¿ç”¨segmentå¼€å§‹å‰çš„æ­£å¸¸å€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if segment_start_idx > 0:
                        # æ£€æŸ¥segmentå¼€å§‹å‰çš„æ•°æ®ç‚¹ï¼Œæ‰¾åˆ°æœ€åä¸€ä¸ªä½äºé˜ˆå€¼çš„æ•°æ®ç‚¹
                        prev_normal_values = []
                        for i in range(segment_start_idx - 1, -1, -1):
                            if values[i] <= abs_threshold:
                                prev_normal_values.append(values[i])
                            if len(prev_normal_values) >= 3:  # è‡³å°‘3ä¸ªæ­£å¸¸å€¼
                                break
                        
                        if len(prev_normal_values) > 0:
                            normal_baseline = np.mean(prev_normal_values)
                    
                    # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1æ²¡æœ‰æ‰¾åˆ°æ­£å¸¸å€¼ï¼Œä½¿ç”¨åŸºçº¿çª—å£ä¸­ä½äºé˜ˆå€¼çš„æ•°æ®ç‚¹
                    if normal_baseline is None:
                        baseline_normal_values = baseline_values[baseline_values <= abs_threshold]
                        if len(baseline_normal_values) > 0:
                            normal_baseline = np.mean(baseline_normal_values)
                    
                    # æ–¹æ³•3ï¼šå¦‚æœæ–¹æ³•2ä¹Ÿæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨æ•´ä¸ªæ—¶é—´çª—å£ä¸­ä½äºé˜ˆå€¼çš„æ•°æ®ç‚¹ï¼ˆæ’é™¤å¼‚å¸¸æ®µï¼‰
                    if normal_baseline is None:
                        # æ’é™¤æ‰€æœ‰å¼‚å¸¸æ®µçš„æ•°æ®ç‚¹
                        all_normal_indices = []
                        for i in range(len(values)):
                            if values[i] <= abs_threshold:
                                # æ£€æŸ¥æ˜¯å¦åœ¨å¼‚å¸¸æ®µä¸­
                                is_in_segment = False
                                for seg in continuous_segments:
                                    if i in seg:
                                        is_in_segment = True
                                        break
                                if not is_in_segment:
                                    all_normal_indices.append(i)
                        
                        if len(all_normal_indices) > 0:
                            normal_baseline = np.mean(values[all_normal_indices])
                    
                    # æ–¹æ³•4ï¼šå¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½æ²¡æœ‰æ‰¾åˆ°æ­£å¸¸å€¼ï¼Œä½¿ç”¨é˜ˆå€¼ä½œä¸ºå‚è€ƒ
                    if normal_baseline is None or normal_baseline <= 0:
                        normal_baseline = abs_threshold
                    
                    # è®¡ç®—ç›¸å¯¹äºæ­£å¸¸åŸºçº¿çš„åç¦»ç™¾åˆ†æ¯”ï¼ˆæ›´èƒ½åæ˜ å®é™…å¼‚å¸¸ç¨‹åº¦ï¼‰
                    # å¦‚æœæ­£å¸¸åŸºçº¿å€¼å¾ˆå°ï¼Œä½¿ç”¨get_baseline_paramsçš„é€»è¾‘æ¥å¤„ç†
                    min_baseline_threshold, reference_value = get_baseline_params(kpi_name)
                    
                    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ç›¸å¯¹ç™¾åˆ†æ¯”è¿˜æ˜¯ç»å¯¹å˜åŒ–è½¬æ¢
                    # 1. å¦‚æœæ­£å¸¸åŸºçº¿å€¼å°äºæœ€å°é˜ˆå€¼ï¼Œä½¿ç”¨ç»å¯¹å˜åŒ–è½¬æ¢
                    # 2. å¦‚æœæ­£å¸¸åŸºçº¿å€¼ç›¸å¯¹äºå‚è€ƒå€¼å¾ˆå°ï¼ˆ<10%ï¼‰ï¼Œä¹Ÿä½¿ç”¨ç»å¯¹å˜åŒ–è½¬æ¢
                    #    è¿™æ ·å¯ä»¥é¿å…å½“æ­£å¸¸åŸºçº¿å€¼å¾ˆå°æ—¶ï¼Œç›¸å¯¹ç™¾åˆ†æ¯”å¼‚å¸¸å¤§çš„é—®é¢˜
                    # 3. å¦åˆ™ä½¿ç”¨ç›¸å¯¹ç™¾åˆ†æ¯”ï¼ˆæ›´èƒ½åæ˜ å®é™…å¼‚å¸¸ç¨‹åº¦ï¼‰
                    use_relative = (normal_baseline >= min_baseline_threshold and 
                                   normal_baseline >= reference_value * 0.1)
                    
                    if use_relative:
                        # æ­£å¸¸åŸºçº¿å€¼è¶³å¤Ÿå¤§ï¼Œä½¿ç”¨ç›¸å¯¹ç™¾åˆ†æ¯”
                        deviation_pct = abs((segment_mean - normal_baseline) / normal_baseline * 100)
                    else:
                        # æ­£å¸¸åŸºçº¿å€¼å¾ˆå°ï¼Œä½¿ç”¨ç»å¯¹å˜åŒ–è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                        absolute_change = abs(segment_mean - normal_baseline)
                        deviation_pct = (absolute_change / reference_value) * 100 if reference_value > 0 else 0
                    
                    # å¦‚æœå¹³å‡å€¼è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯å¼‚å¸¸
                    if segment_mean > abs_threshold:
                        # ç›´æ¥ä»DataFrameè·å–æ—¶é—´æˆ³ï¼Œä¿ç•™æ—¶åŒºä¿¡æ¯
                        if segment_start_idx < len(datetime_series):
                            segment_time = datetime_series.iloc[segment_start_idx]
                        else:
                            segment_time = datetime_series.iloc[0]  # fallback
                        
                        anomalies.append({
                            'component_name': component,
                            'faulty_kpi': kpi_name,
                            'fault_start_time': to_iso_shanghai(segment_time),
                            'severity_score': calculate_severity(deviation_pct, max_value),
                            'deviation_pct': float(deviation_pct),  # ç¡®ä¿æ˜¯Python floatç±»å‹
                            'method': 'absolute_threshold',
                            'change_idx': int(segment_start_idx)  # ç¡®ä¿æ˜¯Python intç±»å‹
                        })
            
            return anomalies
        
        # æ£€æµ‹å¼‚å¸¸
        all_anomalies = []
        grouped = df_core.groupby(['cmdb_id', 'kpi_name'])
        
        for (component, kpi_name), group in grouped:
            if len(group) < min(min_data_points_ruptures, min_data_points_zscore):
                continue
                
            anomalies = []
            
            # é¦–å…ˆå°è¯•åŸºäºç»å¯¹é˜ˆå€¼çš„æ£€æµ‹ï¼ˆç”¨äºæ£€æµ‹æŒç»­é«˜å€¼ï¼‰
            absolute_anomalies = detect_with_absolute_threshold(component, kpi_name, group)
            anomalies.extend(absolute_anomalies)
            
            if method in ['ruptures', 'both']:
                ruptures_anomalies = detect_with_ruptures(component, kpi_name, group)
                anomalies.extend(ruptures_anomalies)
            
            if method in ['zscore', 'both']:
                zscore_anomalies = detect_with_zscore(component, kpi_name, group)
                anomalies.extend(zscore_anomalies)
            
            # å¦‚æœä½¿ç”¨bothæ–¹æ³•ï¼Œå»é‡ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„ï¼‰
            if method == 'both' and len(anomalies) > 1:
                anomalies = sorted(anomalies, key=lambda x: x.get('change_idx', 0))
                ruptures_results = [a for a in anomalies if a['method'] == 'ruptures']
                if ruptures_results:
                    anomalies = [ruptures_results[0]]
                else:
                    anomalies = [anomalies[0]]
            
            all_anomalies.extend(anomalies)
        
        # å»é‡ï¼šåŒä¸€ä¸ªç»„ä»¶-æŒ‡æ ‡ç»„åˆåªä¿ç•™ä¸€ä¸ªå¼‚å¸¸ï¼ˆé€‰æ‹©æœ€æ—©çš„ï¼‰
        seen = {}
        for anomaly in all_anomalies:
            key = (anomaly['component_name'], anomaly['faulty_kpi'])
            if key not in seen:
                seen[key] = anomaly
            else:
                existing_time = pd.to_datetime(seen[key]['fault_start_time'])
                current_time = pd.to_datetime(anomaly['fault_start_time'])
                if current_time < existing_time:
                    seen[key] = anomaly
        
        final_anomalies = list(seen.values())
        
        # æŒ‰ç…§åç¦»ç¨‹åº¦ä»é«˜åˆ°ä½æ’åº
        final_anomalies = sorted(final_anomalies, key=lambda x: x.get('deviation_pct', 0), reverse=True)
        
        # å¦‚æœæŒ‡å®šäº†topå‚æ•°ä¸”methodä¸æ˜¯bothï¼Œé™åˆ¶è¿”å›æ•°é‡
        if method != 'both' and top > 0:
            final_anomalies = final_anomalies[:top]
        
        # è½¬æ¢numpy/pandasç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œä»¥ä¾¿JSONåºåˆ—åŒ–
        def convert_to_native_types(obj):
            """é€’å½’è½¬æ¢numpy/pandasç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
            # æ£€æŸ¥numpyæ•´æ•°ç±»å‹
            if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
                return int(obj)
            # æ£€æŸ¥numpyæµ®ç‚¹ç±»å‹
            elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
                return float(obj)
            # æ£€æŸ¥numpyæ•°ç»„
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            # æ£€æŸ¥pandasçš„NAå€¼
            elif pd.isna(obj):
                return None
            # æ£€æŸ¥å­—å…¸
            elif isinstance(obj, dict):
                return {key: convert_to_native_types(value) for key, value in obj.items()}
            # æ£€æŸ¥åˆ—è¡¨å’Œå…ƒç»„
            elif isinstance(obj, (list, tuple)):
                return [convert_to_native_types(item) for item in obj]
            # å…¶ä»–ç±»å‹ç›´æ¥è¿”å›
            else:
                return obj
        
        # è½¬æ¢æ‰€æœ‰å¼‚å¸¸æ•°æ®
        final_anomalies = [convert_to_native_types(anomaly) for anomaly in final_anomalies]
        
        return json.dumps(final_anomalies, ensure_ascii=False, indent=2)
